{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11122,
     "status": "ok",
     "timestamp": 1745258894414,
     "user": {
      "displayName": "MohamedAmine Dridi",
      "userId": "10514399361138374798"
     },
     "user_tz": -60
    },
    "id": "KFoVh3YBRJzk",
    "outputId": "25290e1a-1fbe-441c-ba67-b609b65e4c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
      "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.1.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install netCDF4 xarray matplotlib pandas scikit-learn tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to access your files\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745259671846,
     "user": {
      "displayName": "MohamedAmine Dridi",
      "userId": "10514399361138374798"
     },
     "user_tz": -60
    },
    "id": "cHp-K1OSR2Jy",
    "outputId": "abbf65d9-f53a-4d66-e352-c597a6bc8d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunisia PFZ Model - Starting processing pipeline...\n",
      "Base directory: /content/drive/MyDrive/Tunisia_PFZ_Model/\n",
      "Tunisia region: Lat 30°-38°N, Lon 7°-12°E\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# SECTION 1: SETUP AND CONFIGURATION\n",
    "#############################################################\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Base directory for the Tunisia PFZ Model\n",
    "base_dir = '/content/drive/MyDrive/Tunisia_PFZ_Model/'\n",
    "\n",
    "# Create results directories\n",
    "results_dir = os.path.join(base_dir, 'Results')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "processed_dir = os.path.join(results_dir, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "\n",
    "analysis_dir = os.path.join(results_dir, 'Analysis')\n",
    "if not os.path.exists(analysis_dir):\n",
    "    os.makedirs(analysis_dir)\n",
    "\n",
    "model_dir = os.path.join(results_dir, 'Models')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Tunisia bounding box: lat: 30-38°N, lon: 7-12°E\n",
    "tunisia_lat_min, tunisia_lat_max = 30, 38\n",
    "tunisia_lon_min, tunisia_lon_max = 7, 12\n",
    "\n",
    "print(\"Tunisia PFZ Model - Starting processing pipeline...\")\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Tunisia region: Lat {tunisia_lat_min}°-{tunisia_lat_max}°N, Lon {tunisia_lon_min}°-{tunisia_lon_max}°E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBRMXFtSiG-S"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 2: FILE READING FUNCTIONS\n",
    "#############################################################\n",
    "\n",
    "def read_satellite_files(directory):\n",
    "    \"\"\"Read all NetCDF files from directory\"\"\"\n",
    "    files = glob.glob(os.path.join(directory, '*.nc'))\n",
    "    data_dict = {}\n",
    "\n",
    "    print(f\"Found {len(files)} files in {directory}\")\n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"Reading file: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Try multiple engines to handle different NetCDF formats\n",
    "            for engine in [None, 'netcdf4', 'h5netcdf', 'scipy']:\n",
    "                try:\n",
    "                    if engine is None:\n",
    "                        data = xr.open_dataset(file)\n",
    "                    else:\n",
    "                        data = xr.open_dataset(file, engine=engine)\n",
    "                    # If we get here, we successfully opened the file\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if engine == 'scipy':  # Last attempt failed\n",
    "                        raise e\n",
    "                    continue\n",
    "\n",
    "            # Store in dictionary with filename as key\n",
    "            data_dict[filename] = data\n",
    "\n",
    "            # Print basic info\n",
    "            print(f\"Dimensions: {dict(data.sizes)}\")\n",
    "            print(f\"Variables: {list(data.variables)}\")\n",
    "            print(\"-------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Read all data from each directory\n",
    "def read_all_data():\n",
    "    print(\"Reading SST monthly data...\")\n",
    "    sst_mo_data = read_satellite_files(os.path.join(base_dir, 'SST_MO'))\n",
    "\n",
    "    print(\"\\nReading CHL monthly data...\")\n",
    "    chl_mo_data = read_satellite_files(os.path.join(base_dir, 'CHL_MO'))\n",
    "\n",
    "    print(\"\\nReading SST daily data...\")\n",
    "    sst_dl_data = read_satellite_files(os.path.join(base_dir, 'SST_DL'))\n",
    "\n",
    "    print(\"\\nReading CHL daily data...\")\n",
    "    chl_dl_data = read_satellite_files(os.path.join(base_dir, 'CHL_DL'))\n",
    "\n",
    "    return sst_mo_data, chl_mo_data, sst_dl_data, chl_dl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXbJBNiYiNK-"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 3: DATA CROPPING FUNCTIONS\n",
    "#############################################################\n",
    "\n",
    "def crop_to_tunisia(data_dict):\n",
    "    \"\"\"Crop satellite data to Tunisia region\"\"\"\n",
    "    cropped_data = {}\n",
    "\n",
    "    for filename, data in data_dict.items():\n",
    "        print(f\"Cropping file: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Get indices corresponding to Tunisia region\n",
    "            lat = data['lat'].values\n",
    "            lon = data['lon'].values\n",
    "\n",
    "            # Find indices for Tunisia region\n",
    "            lat_indices = np.where((lat >= tunisia_lat_min) & (lat <= tunisia_lat_max))[0]\n",
    "            lon_indices = np.where((lon >= tunisia_lon_min) & (lon <= tunisia_lon_max))[0]\n",
    "\n",
    "            if len(lat_indices) == 0 or len(lon_indices) == 0:\n",
    "                print(f\"No data points found in Tunisia region for {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Get min/max indices\n",
    "            lat_min_idx, lat_max_idx = min(lat_indices), max(lat_indices)\n",
    "            lon_min_idx, lon_max_idx = min(lon_indices), max(lon_indices)\n",
    "\n",
    "            # Create a new dataset with just the Tunisia region\n",
    "            data_cropped = data.isel(lat=slice(lat_min_idx, lat_max_idx + 1),\n",
    "                                    lon=slice(lon_min_idx, lon_max_idx + 1))\n",
    "\n",
    "            # Store in dictionary\n",
    "            cropped_data[filename] = data_cropped\n",
    "\n",
    "            # Print dimensions after cropping\n",
    "            print(f\"Cropped dimensions: {dict(data_cropped.sizes)}\")\n",
    "\n",
    "            # Save cropped file\n",
    "            result_filename = f\"Tunisia_{filename}\"\n",
    "            result_path = os.path.join(results_dir, result_filename)\n",
    "\n",
    "            # Handle potential error in saving\n",
    "            try:\n",
    "                data_cropped.to_netcdf(result_path)\n",
    "                print(f\"Saved Tunisia data to {result_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving NetCDF file: {e}\")\n",
    "                print(\"Trying alternative save method...\")\n",
    "                try:\n",
    "                    # Try with scipy engine which is more forgiving\n",
    "                    data_cropped.to_netcdf(result_path, engine='scipy')\n",
    "                    print(f\"Saved Tunisia data to {result_path} using scipy engine\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Could not save file: {e2}\")\n",
    "\n",
    "            print(\"-------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cropping file {filename}: {e}\")\n",
    "\n",
    "    return cropped_data\n",
    "\n",
    "# Crop all data to Tunisia region\n",
    "def crop_all_data(sst_mo_data, chl_mo_data, sst_dl_data, chl_dl_data):\n",
    "    print(\"\\nCropping SST monthly data to Tunisia...\")\n",
    "    sst_mo_tunisia = crop_to_tunisia(sst_mo_data)\n",
    "\n",
    "    print(\"\\nCropping CHL monthly data to Tunisia...\")\n",
    "    chl_mo_tunisia = crop_to_tunisia(chl_mo_data)\n",
    "\n",
    "    print(\"\\nCropping SST daily data to Tunisia...\")\n",
    "    sst_dl_tunisia = crop_to_tunisia(sst_dl_data)\n",
    "\n",
    "    print(\"\\nCropping CHL daily data to Tunisia...\")\n",
    "    chl_dl_tunisia = crop_to_tunisia(chl_dl_data)\n",
    "\n",
    "    return sst_mo_tunisia, chl_mo_tunisia, sst_dl_tunisia, chl_dl_tunisia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqfKkGc6ibzh"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 4: DATA CLASSIFICATION FUNCTIONS\n",
    "#############################################################\n",
    "\n",
    "def classify_fishing_zone(row):\n",
    "    \"\"\"Classify fishing zone potential based on SST and chlorophyll values\"\"\"\n",
    "    sst = row['sst']\n",
    "    chl = row['chlorophyll']\n",
    "\n",
    "    # For Tunisia, typical SST ranges from ~13°C in winter to ~30°C in summer\n",
    "    # Chlorophyll concentration varies widely, but typically 0.1-0.5 mg/m³ is good\n",
    "\n",
    "    # Season-based classification (using the date to determine season)\n",
    "    date_str = str(row['date'])\n",
    "    # Handle both formats: 'Monthly_YYYYMM', 'Daily_YYYYMMDD', or just 'YYYYMM'\n",
    "    if 'Monthly_' in date_str:\n",
    "        month = int(date_str.replace('Monthly_', '')[-6:-4])\n",
    "    elif 'Daily_' in date_str:\n",
    "        month = int(date_str.replace('Daily_', '')[-8:-6])\n",
    "    else:\n",
    "        month = int(date_str[-6:-4]) if len(date_str) >= 6 else int(date_str[-2:])\n",
    "\n",
    "    # Different criteria by season\n",
    "    if month in [6, 7, 8]:  # Summer\n",
    "        # Summer: higher temperatures, prefer moderate chlorophyll\n",
    "        if chl > 0.2 and 24.0 < sst < 29.0:\n",
    "            return 'HIGH'\n",
    "        elif chl > 0.1 and 22.0 < sst < 30.0:\n",
    "            return 'MEDIUM'\n",
    "        else:\n",
    "            return 'LOW'\n",
    "    elif month in [12, 1, 2]:  # Winter\n",
    "        # Winter: prefer warmer spots with high chlorophyll\n",
    "        if chl > 0.4 and sst > 16.0:\n",
    "            return 'HIGH'\n",
    "        elif chl > 0.25 and sst > 15.0:\n",
    "            return 'MEDIUM'\n",
    "        else:\n",
    "            return 'LOW'\n",
    "    else:  # Spring/Fall\n",
    "        # Spring/Fall transition: balanced conditions\n",
    "        if chl > 0.3 and 18.0 < sst < 26.0:\n",
    "            return 'HIGH'\n",
    "        elif chl > 0.15 and 16.0 < sst < 28.0:\n",
    "            return 'MEDIUM'\n",
    "        else:\n",
    "            return 'LOW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWT8WZ_YifWs"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 5: DATA EXTRACTION FUNCTIONS\n",
    "#############################################################\n",
    "\n",
    "def extract_to_dataframe(sst_data, chl_data, date_label):\n",
    "    \"\"\"Extract SST and Chlorophyll data to a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        # Extract the data variables\n",
    "        sst = sst_data['sst'].values\n",
    "        chlorophyll = chl_data['chlor_a'].values\n",
    "\n",
    "        # Extract coordinates\n",
    "        lat = sst_data['lat'].values\n",
    "        lon = sst_data['lon'].values\n",
    "\n",
    "        print(\"Data shapes:\")\n",
    "        print(f\"SST shape: {sst.shape}\")\n",
    "        print(f\"Chlorophyll shape: {chlorophyll.shape}\")\n",
    "        print(f\"Latitude shape: {lat.shape}\")\n",
    "        print(f\"Longitude shape: {lon.shape}\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        df_data = []\n",
    "\n",
    "        # Process data points\n",
    "        print(\"Processing data points...\")\n",
    "\n",
    "        for y_idx in tqdm(range(len(lat)), desc=\"Processing rows\"):\n",
    "            for x_idx in range(len(lon)):\n",
    "                try:\n",
    "                    # Extract values for this point\n",
    "                    sst_val = sst[y_idx, x_idx]\n",
    "                    chl_val = chlorophyll[y_idx, x_idx]\n",
    "\n",
    "                    # Skip if any value is NaN or invalid\n",
    "                    if (np.isnan(sst_val) or np.isnan(chl_val) or\n",
    "                        isinstance(sst_val, np.ma.core.MaskedConstant) or\n",
    "                        isinstance(chl_val, np.ma.core.MaskedConstant)):\n",
    "                        continue\n",
    "\n",
    "                    # Add to data list\n",
    "                    df_data.append({\n",
    "                        'latitude': float(lat[y_idx]),\n",
    "                        'longitude': float(lon[x_idx]),\n",
    "                        'sst': float(sst_val),\n",
    "                        'chlorophyll': float(chl_val),\n",
    "                        'date': date_label\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Skip problematic points\n",
    "                    continue\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(df_data)\n",
    "        print(f\"Created DataFrame with {len(df)} valid data points\")\n",
    "\n",
    "        # Display head and data statistics\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "\n",
    "        print(\"\\nData statistics:\")\n",
    "        print(df.describe())\n",
    "\n",
    "        # Check for any remaining issues in the data\n",
    "        print(\"\\nChecking for infinite values:\")\n",
    "        print(np.isinf(df[['sst', 'chlorophyll']]).sum())\n",
    "\n",
    "        # Remove any infinities if present\n",
    "        if np.isinf(df[['sst', 'chlorophyll']]).any().any():\n",
    "            print(\"Removing infinite values\")\n",
    "            df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            print(f\"DataFrame now has {len(df)} valid data points\")\n",
    "\n",
    "        # Apply classification to dataset\n",
    "        print(\"\\nClassifying fishing zones...\")\n",
    "        df['FishingZone'] = df.apply(classify_fishing_zone, axis=1)\n",
    "\n",
    "        # Convert to numeric for model training\n",
    "        df['Label'] = df['FishingZone'].map({'HIGH': 2, 'MEDIUM': 1, 'LOW': 0})\n",
    "\n",
    "        # Check distribution of classes\n",
    "        class_counts = df['FishingZone'].value_counts()\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(class_counts)\n",
    "        print(f\"Percentage distribution:\\n{100 * class_counts / len(df)}\")\n",
    "\n",
    "        # Save the DataFrame to CSV\n",
    "        csv_file = os.path.join(processed_dir, f'Tunisia_PFZ_{date_label}.csv')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"Saved DataFrame to {csv_file}\")\n",
    "\n",
    "        # Create visualization of class distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%',\n",
    "                colors=['forestgreen', 'gold', 'red'], startangle=90)\n",
    "        plt.axis('equal')\n",
    "        plt.title(f'Distribution of Fishing Zone Classifications - {date_label}')\n",
    "        plt.savefig(os.path.join(analysis_dir, f'FishingZones_Pie_{date_label}.png'), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Map the classes spatially\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(df['longitude'], df['latitude'], c=df['Label'],\n",
    "                             cmap='viridis', alpha=0.7, s=10)\n",
    "        plt.colorbar(scatter, label='Fishing Zone Potential (0=Low, 1=Medium, 2=High)')\n",
    "        plt.title(f'Spatial Distribution of Fishing Zones - {date_label}')\n",
    "        plt.xlabel('Longitude (°E)')\n",
    "        plt.ylabel('Latitude (°N)')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.savefig(os.path.join(analysis_dir, f'FishingZones_Map_{date_label}.png'), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data to DataFrame: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jqe6AMAMijG7"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 6: PAIRED DATA PROCESSING\n",
    "#############################################################\n",
    "\n",
    "def find_matching_pairs(sst_files_dict, chl_files_dict, is_monthly=True):\n",
    "    \"\"\"Find matching SST and CHL file pairs\"\"\"\n",
    "    pairs = []\n",
    "\n",
    "    for sst_filename in sst_files_dict:\n",
    "        if is_monthly:\n",
    "            # Extract date from monthly filename (AQUA_MODIS.YYYYMM_YYYYMMDD...)\n",
    "            sst_date = sst_filename.split('.')[1].split('_')[0]\n",
    "        else:\n",
    "            # Extract date from daily filename (JPSS1_VIIRS.YYYYMMDD...)\n",
    "            sst_date = sst_filename.split('.')[1]\n",
    "\n",
    "        for chl_filename in chl_files_dict:\n",
    "            if is_monthly:\n",
    "                chl_date = chl_filename.split('.')[1].split('_')[0]\n",
    "            else:\n",
    "                chl_date = chl_filename.split('.')[1]\n",
    "\n",
    "            if sst_date == chl_date:\n",
    "                pairs.append((sst_filename, chl_filename, sst_date))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def process_paired_data(sst_mo_tunisia, chl_mo_tunisia, sst_dl_tunisia, chl_dl_tunisia):\n",
    "    \"\"\"Process paired SST and CHL data\"\"\"\n",
    "    # Find matching monthly pairs\n",
    "    monthly_pairs = find_matching_pairs(sst_mo_tunisia, chl_mo_tunisia, is_monthly=True)\n",
    "    print(f\"\\nFound {len(monthly_pairs)} matching monthly pairs\")\n",
    "\n",
    "    # Process each monthly pair\n",
    "    monthly_dfs = {}\n",
    "    for sst_filename, chl_filename, date in monthly_pairs:\n",
    "        print(f\"\\nProcessing monthly pair for {date}\")\n",
    "        prefix = \"Monthly_\" if len(date) == 6 else \"\"  # Add prefix for clarity\n",
    "        monthly_dfs[date] = extract_to_dataframe(\n",
    "            sst_mo_tunisia[sst_filename],\n",
    "            chl_mo_tunisia[chl_filename],\n",
    "            f\"{prefix}{date}\"\n",
    "        )\n",
    "\n",
    "    # Find matching daily pairs\n",
    "    daily_pairs = find_matching_pairs(sst_dl_tunisia, chl_dl_tunisia, is_monthly=False)\n",
    "    print(f\"\\nFound {len(daily_pairs)} matching daily pairs\")\n",
    "\n",
    "    # Process each daily pair\n",
    "    daily_dfs = {}\n",
    "    for sst_filename, chl_filename, date in daily_pairs:\n",
    "        print(f\"\\nProcessing daily pair for {date}\")\n",
    "        daily_dfs[date] = extract_to_dataframe(\n",
    "            sst_dl_tunisia[sst_filename],\n",
    "            chl_dl_tunisia[chl_filename],\n",
    "            f\"Daily_{date}\"\n",
    "        )\n",
    "\n",
    "    return monthly_dfs, daily_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c99BIQMbiozR"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 7: PFZ ANALYSIS FUNCTIONS\n",
    "#############################################################\n",
    "\n",
    "def calculate_pfz_index(df, output_prefix):\n",
    "    \"\"\"Calculate PFZ index and create visualizations\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No data points available for PFZ calculation.\")\n",
    "        return None\n",
    "\n",
    "    # Create a grid for spatial analysis\n",
    "    lat_min, lat_max = df['latitude'].min(), df['latitude'].max()\n",
    "    lon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n",
    "\n",
    "    # Create grid with 0.05 degree resolution\n",
    "    lat_resolution = 0.05\n",
    "    lon_resolution = 0.05\n",
    "    lat_bins = np.arange(lat_min, lat_max + lat_resolution, lat_resolution)\n",
    "    lon_bins = np.arange(lon_min, lon_max + lon_resolution, lon_resolution)\n",
    "\n",
    "    # Bin the data to create a grid\n",
    "    grid_sst = np.zeros((len(lat_bins)-1, len(lon_bins)-1))\n",
    "    grid_chl = np.zeros((len(lat_bins)-1, len(lon_bins)-1))\n",
    "    grid_count = np.zeros((len(lat_bins)-1, len(lon_bins)-1))\n",
    "\n",
    "    # Populate the grid\n",
    "    for _, row in df.iterrows():\n",
    "        lat_idx = int((row['latitude'] - lat_min) / lat_resolution)\n",
    "        lon_idx = int((row['longitude'] - lon_min) / lon_resolution)\n",
    "\n",
    "        if 0 <= lat_idx < len(lat_bins)-1 and 0 <= lon_idx < len(lon_bins)-1:\n",
    "            grid_sst[lat_idx, lon_idx] += row['sst']\n",
    "            grid_chl[lat_idx, lon_idx] += row['chlorophyll']\n",
    "            grid_count[lat_idx, lon_idx] += 1\n",
    "\n",
    "    # Average values where there are multiple points\n",
    "    mask = grid_count > 0\n",
    "    grid_sst[mask] = grid_sst[mask] / grid_count[mask]\n",
    "    grid_chl[mask] = grid_chl[mask] / grid_count[mask]\n",
    "\n",
    "    # Set zeros to NaN for better visualization\n",
    "    grid_sst[~mask] = np.nan\n",
    "    grid_chl[~mask] = np.nan\n",
    "\n",
    "    # Calculate SST gradient\n",
    "    dy, dx = np.gradient(grid_sst)\n",
    "    gradient_magnitude = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "    # Normalize values between 0 and 1\n",
    "    norm_gradient = np.zeros_like(gradient_magnitude)\n",
    "    norm_chl = np.zeros_like(grid_chl)\n",
    "\n",
    "    if np.nanmax(gradient_magnitude) > np.nanmin(gradient_magnitude):\n",
    "        norm_gradient = (gradient_magnitude - np.nanmin(gradient_magnitude)) / (np.nanmax(gradient_magnitude) - np.nanmin(gradient_magnitude))\n",
    "\n",
    "    if np.nanmax(grid_chl) > np.nanmin(grid_chl):\n",
    "        norm_chl = (grid_chl - np.nanmin(grid_chl)) / (np.nanmax(grid_chl) - np.nanmin(grid_chl))\n",
    "\n",
    "    # Calculate PFZ index (weighted sum)\n",
    "    pfz_index = 0.6 * norm_gradient + 0.4 * norm_chl\n",
    "\n",
    "    # Create visualizations\n",
    "    create_pfz_visualizations(\n",
    "        lat_bins[:-1], lon_bins[:-1],\n",
    "        grid_sst, grid_chl, norm_gradient, pfz_index,\n",
    "        output_prefix\n",
    "    )\n",
    "\n",
    "    # Identify potential fishing zones\n",
    "    identify_potential_fishing_zones(\n",
    "        lat_bins[:-1], lon_bins[:-1],\n",
    "        pfz_index, grid_sst, grid_chl,\n",
    "        output_prefix\n",
    "    )\n",
    "\n",
    "    return pfz_index\n",
    "\n",
    "def create_pfz_visualizations(lat_bins, lon_bins, grid_sst, grid_chl, norm_gradient, pfz_index, output_prefix):\n",
    "    \"\"\"Create visualizations for PFZ analysis\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "    # SST map\n",
    "    im1 = axs[0, 0].pcolormesh(lon_bins, lat_bins, grid_sst, cmap='viridis', shading='auto')\n",
    "    axs[0, 0].set_title(f'Sea Surface Temperature (°C) - {output_prefix}')\n",
    "    axs[0, 0].set_xlabel('Longitude (°E)')\n",
    "    axs[0, 0].set_ylabel('Latitude (°N)')\n",
    "    plt.colorbar(im1, ax=axs[0, 0], label='Temperature (°C)')\n",
    "\n",
    "    # Chlorophyll map\n",
    "    im2 = axs[0, 1].pcolormesh(lon_bins, lat_bins, grid_chl, cmap='plasma', shading='auto')\n",
    "    axs[0, 1].set_title(f'Chlorophyll-a Concentration (mg/m³) - {output_prefix}')\n",
    "    axs[0, 1].set_xlabel('Longitude (°E)')\n",
    "    axs[0, 1].set_ylabel('Latitude (°N)')\n",
    "    plt.colorbar(im2, ax=axs[0, 1], label='Chlorophyll-a (mg/m³)')\n",
    "\n",
    "    # SST Gradient map\n",
    "    im3 = axs[1, 0].pcolormesh(lon_bins, lat_bins, norm_gradient, cmap='hot_r', shading='auto')\n",
    "    axs[1, 0].set_title(f'Normalized SST Gradient - {output_prefix}')\n",
    "    axs[1, 0].set_xlabel('Longitude (°E)')\n",
    "    axs[1, 0].set_ylabel('Latitude (°N)')\n",
    "    plt.colorbar(im3, ax=axs[1, 0], label='Gradient Magnitude (normalized)')\n",
    "\n",
    "    # PFZ map - custom colormap\n",
    "    pfz_cmap = LinearSegmentedColormap.from_list('pfz',\n",
    "                                               [(0, 'darkblue'),\n",
    "                                                (0.25, 'blue'),\n",
    "                                                (0.5, 'lightgreen'),\n",
    "                                                (0.75, 'yellow'),\n",
    "                                                (1, 'red')])\n",
    "\n",
    "    im4 = axs[1, 1].pcolormesh(lon_bins, lat_bins, pfz_index, cmap=pfz_cmap, shading='auto')\n",
    "    axs[1, 1].set_title(f'Potential Fishing Zone Index - {output_prefix}')\n",
    "    axs[1, 1].set_xlabel('Longitude (°E)')\n",
    "    axs[1, 1].set_ylabel('Latitude (°N)')\n",
    "    cbar = plt.colorbar(im4, ax=axs[1, 1], label='PFZ Index')\n",
    "    cbar.set_ticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    cbar.set_ticklabels(['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "    # Add Tunisia coastline reference\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xlim(min(lon_bins), max(lon_bins))\n",
    "        ax.set_ylim(min(lat_bins), max(lat_bins))\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig_path = os.path.join(analysis_dir, f'PFZ_Analysis_{output_prefix}.png')\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved analysis figure to {fig_path}\")\n",
    "\n",
    "    # Close the figure to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "def identify_potential_fishing_zones(lat_bins, lon_bins, pfz_index, grid_sst, grid_chl, output_prefix):\n",
    "    \"\"\"Identify and save potential fishing zones\"\"\"\n",
    "    pfz_df = pd.DataFrame()\n",
    "\n",
    "    for lat_idx in range(len(lat_bins)):\n",
    "        for lon_idx in range(len(lon_bins)):\n",
    "            if lat_idx < pfz_index.shape[0] and lon_idx < pfz_index.shape[1]:\n",
    "                if not np.isnan(pfz_index[lat_idx, lon_idx]) and pfz_index[lat_idx, lon_idx] > 0.7:  # High PFZ index threshold\n",
    "                    pfz_df = pfz_df._append({\n",
    "                        'latitude': lat_bins[lat_idx],\n",
    "                        'longitude': lon_bins[lon_idx],\n",
    "                        'pfz_index': pfz_index[lat_idx, lon_idx],\n",
    "                        'sst': grid_sst[lat_idx, lon_idx],\n",
    "                        'chlorophyll': grid_chl[lat_idx, lon_idx]\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "    # Sort by PFZ index (highest first)\n",
    "    if len(pfz_df) > 0:\n",
    "        pfz_df = pfz_df.sort_values('pfz_index', ascending=False)\n",
    "\n",
    "        # Save the top PFZ locations\n",
    "        pfz_path = os.path.join(analysis_dir, f'PFZ_Locations_{output_prefix}.csv')\n",
    "        pfz_df.to_csv(pfz_path, index=False)\n",
    "        print(f\"Saved top {len(pfz_df)} potential fishing zones to {pfz_path}\")\n",
    "\n",
    "        # Print the top 5 PFZ locations\n",
    "        print(\"\\nTop 5 Potential Fishing Zones:\")\n",
    "        for idx, row in pfz_df.head(5).iterrows():\n",
    "            print(f\"Location {idx+1}: Lat {row['latitude']:.4f}°N, Lon {row['longitude']:.4f}°E (PFZ Index: {row['pfz_index']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nNo high-potential fishing zones identified.\")\n",
    "\n",
    "    return pfz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE3QR_MdjumD"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 8: MACHINE LEARNING MODEL\n",
    "#############################################################\n",
    "\n",
    "def train_pfz_prediction_model(all_dfs):\n",
    "    \"\"\"Train machine learning models to predict fishing zones based on SST and chlorophyll\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    print(\"\\n--- TRAINING PFZ PREDICTION MODEL ---\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if len(all_dfs) == 0:\n",
    "        print(\"No valid dataframes available for model training\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine all valid dataframes\n",
    "    combined_df = pd.concat([df for df in all_dfs.values() if df is not None and len(df) > 0])\n",
    "    print(f\"Combined dataset has {len(combined_df)} data points\")\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = combined_df[['sst', 'chlorophyll']]\n",
    "\n",
    "    # Check for and handle any remaining NaN or infinite values\n",
    "    if X.isna().any().any() or np.isinf(X).any().any():\n",
    "        print(\"Removing any remaining NaN or infinite values\")\n",
    "        combined_df = combined_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        X = combined_df[['sst', 'chlorophyll']]\n",
    "        print(f\"Dataset now has {len(combined_df)} valid points\")\n",
    "\n",
    "    # Normalize features\n",
    "    X_scaled = preprocessing.scale(X)\n",
    "    X_normalized = preprocessing.normalize(X_scaled, norm='l2')\n",
    "    y = combined_df['Label']\n",
    "\n",
    "    # One-hot encode labels for neural network\n",
    "    y_categorical = to_categorical(y, num_classes=3)\n",
    "\n",
    "    # Split data for model training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y,\n",
    "        test_size=0.2, random_state=42,\n",
    "        stratify=y  # Ensure balanced classes\n",
    "    )\n",
    "\n",
    "    # Also prepare categorical targets for neural network\n",
    "    _, _, y_train_cat, y_test_cat = train_test_split(\n",
    "        X_normalized, y_categorical,\n",
    "        test_size=0.2, random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {len(X_train)} samples\")\n",
    "    print(f\"Testing set size: {len(X_test)} samples\")\n",
    "\n",
    "    # Check class distribution\n",
    "    train_class_counts = pd.Series(y_train).value_counts()\n",
    "    test_class_counts = pd.Series(y_test).value_counts()\n",
    "\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_class_counts)\n",
    "    print(f\"Percentage: {100 * train_class_counts / len(y_train)}\")\n",
    "\n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(test_class_counts)\n",
    "    print(f\"Percentage: {100 * test_class_counts / len(y_test)}\")\n",
    "\n",
    "    # Create class distribution visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie(train_class_counts, labels=['Low', 'Medium', 'High'], autopct='%1.1f%%',\n",
    "           colors=['skyblue', 'lightgreen', 'salmon'])\n",
    "    plt.title('Training Set Class Distribution')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(test_class_counts, labels=['Low', 'Medium', 'High'], autopct='%1.1f%%',\n",
    "           colors=['skyblue', 'lightgreen', 'salmon'])\n",
    "    plt.title('Test Set Class Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    class_dist_path = os.path.join(analysis_dir, 'PFZ_Class_Distribution.png')\n",
    "    plt.savefig(class_dist_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 1. Random Forest Model - without grid search\n",
    "    print(\"\\nTraining Random Forest Classifier (simplified version)...\")\n",
    "\n",
    "    # Use a simpler model with default parameters\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and metrics\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "    rf_f1 = f1_score(y_test, rf_pred, average='weighted')\n",
    "\n",
    "    print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "    print(f\"Random Forest F1 Score (weighted): {rf_f1:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': ['SST', 'Chlorophyll'],\n",
    "        'importance': rf.feature_importances_\n",
    "    })\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='feature', y='importance', data=feature_importances)\n",
    "    plt.title('Feature Importance - Random Forest')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    fi_path = os.path.join(analysis_dir, 'PFZ_RF_Feature_Importance.png')\n",
    "    plt.savefig(fi_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion matrix\n",
    "    rf_cm = confusion_matrix(y_test, rf_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Low', 'Medium', 'High'],\n",
    "               yticklabels=['Low', 'Medium', 'High'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    cm_path = os.path.join(analysis_dir, 'PFZ_RF_Confusion_Matrix.png')\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Classification report\n",
    "    rf_report = classification_report(y_test, rf_pred, target_names=['Low', 'Medium', 'High'])\n",
    "    print(\"\\nRandom Forest Classification Report:\")\n",
    "    print(rf_report)\n",
    "\n",
    "    # 2. Neural Network Model - simpler and faster\n",
    "    print(\"\\nTraining Neural Network (simplified version)...\")\n",
    "\n",
    "    # Define model architecture - simpler network\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(2,)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train model - fewer epochs\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=50,  # Reduced from 100\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    nn_loss, nn_accuracy = model.evaluate(X_test, y_test_cat)\n",
    "    print(f\"Neural Network Accuracy: {nn_accuracy:.4f}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    history_path = os.path.join(analysis_dir, 'PFZ_NN_Training_History.png')\n",
    "    plt.savefig(history_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Generate predictions\n",
    "    nn_pred_prob = model.predict(X_test)\n",
    "    nn_pred = np.argmax(nn_pred_prob, axis=1)\n",
    "    nn_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    nn_f1 = f1_score(nn_true, nn_pred, average='weighted')\n",
    "    print(f\"Neural Network F1 Score (weighted): {nn_f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    nn_cm = confusion_matrix(nn_true, nn_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Low', 'Medium', 'High'],\n",
    "               yticklabels=['Low', 'Medium', 'High'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Neural Network')\n",
    "    nn_cm_path = os.path.join(analysis_dir, 'PFZ_NN_Confusion_Matrix.png')\n",
    "    plt.savefig(nn_cm_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Classification report\n",
    "    nn_report = classification_report(nn_true, nn_pred, target_names=['Low', 'Medium', 'High'])\n",
    "    print(\"\\nNeural Network Classification Report:\")\n",
    "    print(nn_report)\n",
    "\n",
    "    # Compare models\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    model_names = ['Random Forest', 'Neural Network']\n",
    "    accuracies = [rf_accuracy, nn_accuracy]\n",
    "    f1_scores = [rf_f1, nn_f1]\n",
    "\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, accuracies, width, label='Accuracy')\n",
    "    plt.bar(x + width/2, f1_scores, width, label='F1 Score')\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, model_names)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    comparison_path = os.path.join(analysis_dir, 'PFZ_Model_Comparison.png')\n",
    "    plt.savefig(comparison_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save classification reports\n",
    "    report_path = os.path.join(analysis_dir, 'PFZ_Classification_Reports.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"TUNISIA PFZ MODEL - CLASSIFICATION REPORTS\\n\")\n",
    "        f.write(\"=======================================\\n\\n\")\n",
    "        f.write(f\"Total data points: {len(combined_df)}\\n\")\n",
    "        f.write(f\"Training samples: {len(X_train)}\\n\")\n",
    "        f.write(f\"Testing samples: {len(X_test)}\\n\\n\")\n",
    "\n",
    "        f.write(\"RANDOM FOREST MODEL\\n\")\n",
    "        f.write(\"------------------\\n\")\n",
    "        f.write(f\"Parameters: n_estimators=100, max_depth=10\\n\")\n",
    "        f.write(f\"Accuracy: {rf_accuracy:.4f}\\n\")\n",
    "        f.write(f\"F1 Score (weighted): {rf_f1:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(rf_report)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"NEURAL NETWORK MODEL\\n\")\n",
    "        f.write(\"------------------\\n\")\n",
    "        f.write(f\"Accuracy: {nn_accuracy:.4f}\\n\")\n",
    "        f.write(f\"F1 Score (weighted): {nn_f1:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(nn_report)\n",
    "\n",
    "    print(f\"Saved classification reports to {report_path}\")\n",
    "\n",
    "    # Save models\n",
    "    # Random Forest\n",
    "    from joblib import dump\n",
    "    rf_model_path = os.path.join(model_dir, 'random_forest_model.joblib')\n",
    "    dump(rf, rf_model_path)\n",
    "\n",
    "    # Neural Network\n",
    "    nn_model_path = os.path.join(model_dir, 'neural_network_model.keras')\n",
    "    model.save(nn_model_path)\n",
    "\n",
    "    print(f\"Saved Random Forest model to {rf_model_path}\")\n",
    "    print(f\"Saved Neural Network model to {nn_model_path}\")\n",
    "\n",
    "    # Return the better model and a prediction function\n",
    "    if rf_f1 >= nn_f1:\n",
    "        best_model = rf\n",
    "        model_type = \"RandomForest\"\n",
    "\n",
    "        def predict_function(data):\n",
    "            \"\"\"Predict fishing zones using Random Forest model\"\"\"\n",
    "            # Preprocess data\n",
    "            data_scaled = preprocessing.scale(data)\n",
    "            data_normalized = preprocessing.normalize(data_scaled, norm='l2')\n",
    "            # Make predictions\n",
    "            predictions = best_model.predict(data_normalized)\n",
    "            probabilities = best_model.predict_proba(data_normalized)\n",
    "            return predictions, probabilities\n",
    "    else:\n",
    "        best_model = model\n",
    "        model_type = \"NeuralNetwork\"\n",
    "\n",
    "        def predict_function(data):\n",
    "            \"\"\"Predict fishing zones using Neural Network model\"\"\"\n",
    "            # Preprocess data\n",
    "            data_scaled = preprocessing.scale(data)\n",
    "            data_normalized = preprocessing.normalize(data_scaled, norm='l2')\n",
    "            # Make predictions\n",
    "            probabilities = best_model.predict(data_normalized)\n",
    "            predictions = np.argmax(probabilities, axis=1)\n",
    "            return predictions, probabilities\n",
    "\n",
    "    print(f\"\\nThe best model is: {model_type}\")\n",
    "    return best_model, predict_function, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6zy32aomls3"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# SECTION 9: PREDICTION AND VALIDATION\n",
    "#############################################################\n",
    "\n",
    "def generate_pfz_predictions(model, predict_function, model_type, df, date_label):\n",
    "    \"\"\"Generate predictions for a given dataset and validate against ground truth\"\"\"\n",
    "    print(f\"\\nGenerating PFZ predictions for {date_label}...\")\n",
    "\n",
    "    # Extract features\n",
    "    X = df[['sst', 'chlorophyll']].values\n",
    "    y_true = df['Label'].values\n",
    "\n",
    "    # Get predictions\n",
    "    predictions, probabilities = predict_function(X)\n",
    "\n",
    "    # Add predictions to dataframe\n",
    "    df['Predicted_Label'] = predictions\n",
    "    df['Predicted_Zone'] = df['Predicted_Label'].map({0: 'LOW', 1: 'MEDIUM', 2: 'HIGH'})\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions, average='weighted')\n",
    "\n",
    "    print(f\"Prediction accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 score (weighted): {f1:.4f}\")\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Low', 'Medium', 'High'],\n",
    "               yticklabels=['Low', 'Medium', 'High'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'{date_label} - Actual vs Predicted Fishing Zones')\n",
    "    cm_path = os.path.join(analysis_dir, f'PFZ_Prediction_CM_{date_label}.png')\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Map the predicted classes spatially\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(df['longitude'], df['latitude'],\n",
    "                         c=df['Predicted_Label'], cmap='viridis',\n",
    "                         alpha=0.7, s=10)\n",
    "    plt.colorbar(scatter, label='Predicted Fishing Zone Potential (0=Low, 1=Medium, 2=High)')\n",
    "    plt.title(f'Predicted Fishing Zones - {date_label}')\n",
    "    plt.xlabel('Longitude (°E)')\n",
    "    plt.ylabel('Latitude (°N)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    map_path = os.path.join(analysis_dir, f'PFZ_Prediction_Map_{date_label}.png')\n",
    "    plt.savefig(map_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save prediction results\n",
    "    pred_df = df[['latitude', 'longitude', 'sst', 'chlorophyll',\n",
    "                 'FishingZone', 'Label', 'Predicted_Zone', 'Predicted_Label']]\n",
    "    pred_path = os.path.join(processed_dir, f'Tunisia_PFZ_Predictions_{date_label}.csv')\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "    print(f\"Saved prediction results to {pred_path}\")\n",
    "\n",
    "    # Create high potential fishing zone map for operational use\n",
    "    high_potential = df[df['Predicted_Label'] == 2]\n",
    "    if len(high_potential) > 0:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(df['longitude'], df['latitude'], c='lightgray', alpha=0.3, s=5)\n",
    "        plt.scatter(high_potential['longitude'], high_potential['latitude'],\n",
    "                  c='red', alpha=0.8, s=20)\n",
    "        plt.title(f'High Potential Fishing Zones - {date_label}')\n",
    "        plt.xlabel('Longitude (°E)')\n",
    "        plt.ylabel('Latitude (°N)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim(tunisia_lon_min, tunisia_lon_max)\n",
    "        plt.ylim(tunisia_lat_min, tunisia_lat_max)\n",
    "        high_path = os.path.join(analysis_dir, f'High_PFZ_{date_label}.png')\n",
    "        plt.savefig(high_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Saved high potential fishing zones map to {high_path}\")\n",
    "    else:\n",
    "        print(\"No high potential fishing zones identified in this dataset.\")\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 333585,
     "status": "ok",
     "timestamp": 1745261460156,
     "user": {
      "displayName": "MohamedAmine Dridi",
      "userId": "10514399361138374798"
     },
     "user_tz": -60
    },
    "id": "SiYBuf0cnRU6",
    "outputId": "6e6831da-eced-413d-d541-56b1a0cc9e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: READING SATELLITE DATA ---\n",
      "Reading SST monthly data...\n",
      "Found 12 files in /content/drive/MyDrive/Tunisia_PFZ_Model/SST_MO\n",
      "Reading file: AQUA_MODIS.20240201_20240229.L3m.MO.SST.sst.4km.nc\n",
      "Error reading file AQUA_MODIS.20240201_20240229.L3m.MO.SST.sst.4km.nc: Error: /content/drive/MyDrive/Tunisia_PFZ_Model/SST_MO/AQUA_MODIS.20240201_20240229.L3m.MO.SST.sst.4km.nc is not a valid NetCDF 3 file\n",
      "            If this is a NetCDF4 file, you may need to install the\n",
      "            netcdf4 library, e.g.,\n",
      "\n",
      "            $ pip install netcdf4\n",
      "            \n",
      "Reading file: AQUA_MODIS.20240301_20240331.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240101_20240131.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240501_20240531.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240401_20240430.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240601_20240630.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240701_20240731.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240801_20240831.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241001_20241031.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240901_20240930.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241201_20241231.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241101_20241130.L3m.MO.SST.sst.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "\n",
      "Reading CHL monthly data...\n",
      "Found 12 files in /content/drive/MyDrive/Tunisia_PFZ_Model/CHL_MO\n",
      "Reading file: AQUA_MODIS.20240801_20240831.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240101_20240131.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240201_20240229.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240301_20240331.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240401_20240430.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240501_20240531.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240601_20240630.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240701_20240731.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241001_20241031.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20240901_20240930.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241101_20241130.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: AQUA_MODIS.20241201_20241231.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "\n",
      "Reading SST daily data...\n",
      "Found 7 files in /content/drive/MyDrive/Tunisia_PFZ_Model/SST_DL\n",
      "Reading file: JPSS1_VIIRS.20250419.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250418.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250417.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250416.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250415.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250414.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250413.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['sst', 'qual_sst', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "\n",
      "Reading CHL daily data...\n",
      "Found 7 files in /content/drive/MyDrive/Tunisia_PFZ_Model/CHL_DL\n",
      "Reading file: JPSS1_VIIRS.20250419.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250418.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250417.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250416.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250415.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250414.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "Reading file: JPSS1_VIIRS.20250413.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Dimensions: {'lat': 4320, 'lon': 8640, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Variables: ['chlor_a', 'lat', 'lon', 'palette']\n",
      "-------------------------\n",
      "\n",
      "--- STEP 2: CROPPING DATA TO TUNISIA REGION ---\n",
      "\n",
      "Cropping SST monthly data to Tunisia...\n",
      "Cropping file: AQUA_MODIS.20240301_20240331.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240301_20240331.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240101_20240131.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240101_20240131.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240501_20240531.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240501_20240531.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240401_20240430.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240401_20240430.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240601_20240630.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240601_20240630.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240701_20240731.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240701_20240731.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240801_20240831.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240801_20240831.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241001_20241031.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241001_20241031.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240901_20240930.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240901_20240930.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241201_20241231.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241201_20241231.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241101_20241130.L3m.MO.SST.sst.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241101_20241130.L3m.MO.SST.sst.4km.nc\n",
      "-------------------------\n",
      "\n",
      "Cropping CHL monthly data to Tunisia...\n",
      "Cropping file: AQUA_MODIS.20240801_20240831.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240801_20240831.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240101_20240131.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240101_20240131.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240201_20240229.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240201_20240229.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240301_20240331.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240301_20240331.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240401_20240430.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240401_20240430.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240501_20240531.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240501_20240531.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240601_20240630.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240601_20240630.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240701_20240731.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240701_20240731.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241001_20241031.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241001_20241031.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20240901_20240930.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20240901_20240930.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241101_20241130.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241101_20241130.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "Cropping file: AQUA_MODIS.20241201_20241231.L3m.MO.CHL.chlor_a.4km.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_AQUA_MODIS.20241201_20241231.L3m.MO.CHL.chlor_a.4km.nc\n",
      "-------------------------\n",
      "\n",
      "Cropping SST daily data to Tunisia...\n",
      "Cropping file: JPSS1_VIIRS.20250419.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250419.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250418.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250418.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250417.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250417.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250416.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250416.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250415.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250415.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250414.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250414.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250413.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250413.L3m.DAY.SST.sst.4km.NRT.nc\n",
      "-------------------------\n",
      "\n",
      "Cropping CHL daily data to Tunisia...\n",
      "Cropping file: JPSS1_VIIRS.20250419.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250419.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250418.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250418.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250417.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250417.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250416.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250416.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250415.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250415.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250414.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250414.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "Cropping file: JPSS1_VIIRS.20250413.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "Cropped dimensions: {'lat': 192, 'lon': 120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Saved Tunisia data to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Tunisia_JPSS1_VIIRS.20250413.L3m.DAY.CHL.chlor_a.4km.NRT.nc\n",
      "-------------------------\n",
      "\n",
      "--- STEP 3: PROCESSING PAIRED DATA ---\n",
      "\n",
      "Found 11 matching monthly pairs\n",
      "\n",
      "Processing monthly pair for 20240301\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4323.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5769 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  16.315001     0.287264  20240301\n",
      "1  37.979164   7.062506  16.240000     0.289395  20240301\n",
      "2  37.979164   7.104172  16.164999     0.292652  20240301\n",
      "3  37.979164   7.145839  16.094999     0.283431  20240301\n",
      "4  37.979164   7.187506  16.039999     0.281166  20240301\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5769.000000  5769.000000  5769.000000  5769.000000\n",
      "mean     36.245228    10.392307    16.386505     0.720012\n",
      "std       1.474039     1.421338     0.639513     2.038677\n",
      "min      33.020832     7.020839    14.730000     0.141760\n",
      "25%      34.854164     9.437506    15.915000     0.248858\n",
      "50%      36.895832    10.937506    16.209999     0.281840\n",
      "75%      37.479164    11.479173    16.715000     0.327565\n",
      "max      37.979164    11.979173    21.930000    42.434219\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    3813\n",
      "LOW       1861\n",
      "HIGH        95\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    66.094644\n",
      "LOW       32.258624\n",
      "HIGH       1.646733\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240301.csv\n",
      "\n",
      "Processing monthly pair for 20240101\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4614.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5789 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  17.000000     0.280274  20240101\n",
      "1  37.979164   7.062506  17.004999     0.277238  20240101\n",
      "2  37.979164   7.104172  17.070000     0.284001  20240101\n",
      "3  37.979164   7.145839  17.090000     0.284520  20240101\n",
      "4  37.979164   7.187506  17.094999     0.282164  20240101\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5789.000000  5789.000000  5789.000000  5789.000000\n",
      "mean     36.238306    10.395040    16.358435     0.851346\n",
      "std       1.476993     1.418413     0.680068     2.104607\n",
      "min      33.020832     7.020839    13.025000     0.220535\n",
      "25%      34.854164     9.479173    16.014999     0.326390\n",
      "50%      36.895832    10.937506    16.455000     0.358842\n",
      "75%      37.479164    11.479173    16.709999     0.451985\n",
      "max      37.979164    11.979173    18.250000    55.051014\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    4366\n",
      "LOW       1423\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    75.418898\n",
      "LOW       24.581102\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240101.csv\n",
      "\n",
      "Processing monthly pair for 20240501\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:01<00:00, 160.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5740 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  19.029999     0.126862  20240501\n",
      "1  37.979164   7.062506  19.260000     0.125697  20240501\n",
      "2  37.979164   7.104172  19.959999     0.129296  20240501\n",
      "3  37.979164   7.145839  20.004999     0.132227  20240501\n",
      "4  37.979164   7.187506  19.865000     0.134365  20240501\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5740.000000  5740.000000  5740.000000  5740.000000\n",
      "mean     36.252176    10.390475    20.114596     0.735185\n",
      "std       1.470216     1.423675     0.897305     2.768498\n",
      "min      33.020832     7.020839    18.344999     0.082944\n",
      "25%      34.885415     9.437506    19.494999     0.138558\n",
      "50%      36.895832    10.937506    19.814999     0.153171\n",
      "75%      37.520832    11.479173    20.625000     0.196250\n",
      "max      37.979164    11.979173    26.629999    62.710205\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       2462\n",
      "MEDIUM    2287\n",
      "HIGH       991\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       42.891986\n",
      "MEDIUM    39.843206\n",
      "HIGH      17.264808\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240501.csv\n",
      "\n",
      "Processing monthly pair for 20240401\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4490.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5735 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  17.574999     0.160252  20240401\n",
      "1  37.979164   7.062506  17.340000     0.164753  20240401\n",
      "2  37.979164   7.104172  17.350000     0.179476  20240401\n",
      "3  37.979164   7.145839  17.355000     0.174007  20240401\n",
      "4  37.979164   7.187506  17.320000     0.168333  20240401\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5735.000000  5735.000000  5735.000000  5735.000000\n",
      "mean     36.255873    10.392439    17.840170     0.622810\n",
      "std       1.467358     1.422424     0.955714     2.140978\n",
      "min      33.020832     7.020839    16.404999     0.081892\n",
      "25%      34.895832     9.437506    17.115000     0.154124\n",
      "50%      36.895832    10.937506    17.545000     0.166137\n",
      "75%      37.520832    11.479173    18.549999     0.210183\n",
      "max      37.979164    11.979173    21.869999    54.874355\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    3661\n",
      "LOW       1162\n",
      "HIGH       912\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    63.836094\n",
      "LOW       20.261552\n",
      "HIGH      15.902354\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240401.csv\n",
      "\n",
      "Processing monthly pair for 20240601\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4778.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5688 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  23.115000     0.107691  20240601\n",
      "1  37.979164   7.062506  23.184999     0.105442  20240601\n",
      "2  37.979164   7.104172  22.894999     0.112560  20240601\n",
      "3  37.979164   7.145839  22.834999     0.113489  20240601\n",
      "4  37.979164   7.187506  22.809999     0.114113  20240601\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5688.000000  5688.000000  5688.000000  5688.000000\n",
      "mean     36.260298    10.388792    23.108095     0.697095\n",
      "std       1.468582     1.427366     0.853458     2.236595\n",
      "min      33.020832     7.020839    21.029999     0.073988\n",
      "25%      34.895832     9.395839    22.564999     0.109625\n",
      "50%      36.937500    10.937506    22.949999     0.124485\n",
      "75%      37.520832    11.479173    23.389999     0.149689\n",
      "max      37.979164    11.979173    27.559999    31.682673\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       4268\n",
      "HIGH       834\n",
      "MEDIUM     586\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       75.035162\n",
      "HIGH      14.662447\n",
      "MEDIUM    10.302391\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240601.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing monthly pair for 20240701\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4682.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5746 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  26.129999     0.084055  20240701\n",
      "1  37.979164   7.062506  26.115000     0.084388  20240701\n",
      "2  37.979164   7.104172  26.105000     0.083880  20240701\n",
      "3  37.979164   7.145839  26.125000     0.084091  20240701\n",
      "4  37.979164   7.187506  26.119999     0.084742  20240701\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5746.000000  5746.000000  5746.000000  5746.000000\n",
      "mean     36.244872    10.393838    26.549858     0.982676\n",
      "std       1.475461     1.422605     1.071937     3.463466\n",
      "min      33.020832     7.020839    23.824999     0.068108\n",
      "25%      34.854164     9.437506    25.799999     0.094687\n",
      "50%      36.895832    10.937506    26.285000     0.103510\n",
      "75%      37.520832    11.479173    27.019999     0.144657\n",
      "max      37.979164    11.979173    31.174999    52.282475\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       5005\n",
      "MEDIUM     728\n",
      "HIGH        13\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       87.104072\n",
      "MEDIUM    12.669683\n",
      "HIGH       0.226244\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240701.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing monthly pair for 20240801\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4750.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5773 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  27.539999     0.076360  20240801\n",
      "1  37.979164   7.062506  27.535000     0.075609  20240801\n",
      "2  37.979164   7.104172  27.574999     0.076290  20240801\n",
      "3  37.979164   7.145839  27.564999     0.077819  20240801\n",
      "4  37.979164   7.187506  27.529999     0.079200  20240801\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5773.000000  5773.000000  5773.000000  5773.000000\n",
      "mean     36.246062    10.392100    28.364567     1.287635\n",
      "std       1.473591     1.421815     0.958547     3.953377\n",
      "min      33.020832     7.020839    26.724998     0.063483\n",
      "25%      34.854164     9.437506    27.535000     0.089072\n",
      "50%      36.895832    10.937506    28.135000     0.098727\n",
      "75%      37.479164    11.479173    29.054998     0.166314\n",
      "max      37.979164    11.979173    31.824999    44.965679\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       5614\n",
      "MEDIUM     159\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       97.245799\n",
      "MEDIUM     2.754201\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240801.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing monthly pair for 20241001\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 3754.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5780 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  23.514999     0.125374  20241001\n",
      "1  37.979164   7.062506  23.490000     0.125542  20241001\n",
      "2  37.979164   7.104172  23.439999     0.125233  20241001\n",
      "3  37.979164   7.145839  23.424999     0.124658  20241001\n",
      "4  37.979164   7.187506  23.394999     0.124830  20241001\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5780.000000  5780.000000  5780.000000  5780.000000\n",
      "mean     36.239611    10.393669    24.633437     1.377638\n",
      "std       1.476470     1.421096     1.273980     4.598810\n",
      "min      33.020832     7.020839    22.010000     0.100648\n",
      "25%      34.854164     9.437506    23.654999     0.124075\n",
      "50%      36.895832    10.937506    24.344999     0.144102\n",
      "75%      37.479164    11.479173    25.906249     0.326979\n",
      "max      37.979164    11.979173    27.570000    81.774239\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       3029\n",
      "MEDIUM    2154\n",
      "HIGH       597\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       52.404844\n",
      "MEDIUM    37.266436\n",
      "HIGH      10.328720\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20241001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing monthly pair for 20240901\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 2097.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5762 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  26.049999     0.105173  20240901\n",
      "1  37.979164   7.062506  26.010000     0.104158  20240901\n",
      "2  37.979164   7.104172  26.010000     0.103175  20240901\n",
      "3  37.979164   7.145839  25.959999     0.102714  20240901\n",
      "4  37.979164   7.187506  26.004999     0.103340  20240901\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5762.000000  5762.000000  5762.000000  5762.000000\n",
      "mean     36.248979    10.392455    27.013080     1.483195\n",
      "std       1.470981     1.422532     1.212991     4.811378\n",
      "min      33.020832     7.020839    24.809999     0.065116\n",
      "25%      34.854164     9.437506    26.135000     0.115807\n",
      "50%      36.895832    10.937506    26.594999     0.129615\n",
      "75%      37.479164    11.479173    28.118749     0.284656\n",
      "max      37.979164    11.979173    30.494999    79.111908\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "LOW       4742\n",
      "MEDIUM    1017\n",
      "HIGH         3\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "LOW       82.297813\n",
      "MEDIUM    17.650121\n",
      "HIGH       0.052065\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20240901.csv\n",
      "\n",
      "Processing monthly pair for 20241201\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4606.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5795 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  17.469999     0.264480  20241201\n",
      "1  37.979164   7.062506  17.750000     0.269194  20241201\n",
      "2  37.979164   7.104172  17.900000     0.269820  20241201\n",
      "3  37.979164   7.145839  18.010000     0.377011  20241201\n",
      "4  37.979164   7.187506  18.055000     0.413914  20241201\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5795.000000  5795.000000  5795.000000  5795.000000\n",
      "mean     36.234709    10.394696    18.195203     1.232353\n",
      "std       1.479253     1.418744     1.164273     3.813551\n",
      "min      33.020832     7.020839    13.134999     0.153487\n",
      "25%      34.854164     9.479173    17.445000     0.270995\n",
      "50%      36.895832    10.937506    17.945000     0.351198\n",
      "75%      37.479164    11.479173    18.984999     0.620143\n",
      "max      37.979164    11.979173    21.359999    82.118927\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    3611\n",
      "HIGH      2022\n",
      "LOW        162\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    62.312338\n",
      "HIGH      34.892148\n",
      "LOW        2.795513\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20241201.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing monthly pair for 20241101\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4589.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 5771 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll      date\n",
      "0  37.979164   7.020839  20.980000     0.171023  20241101\n",
      "1  37.979164   7.062506  21.080000     0.155617  20241101\n",
      "2  37.979164   7.104172  21.080000     0.165561  20241101\n",
      "3  37.979164   7.145839  21.010000     0.169781  20241101\n",
      "4  37.979164   7.187506  20.994999     0.174501  20241101\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  5771.000000  5771.000000  5771.000000  5771.000000\n",
      "mean     36.242053    10.394041    21.928560     1.459623\n",
      "std       1.476031     1.420513     0.882918     4.488988\n",
      "min      33.020832     7.020839    18.959999     0.138181\n",
      "25%      34.854164     9.458339    21.244999     0.180633\n",
      "50%      36.895832    10.937506    21.635000     0.225446\n",
      "75%      37.479164    11.479173    22.667500     0.554646\n",
      "max      37.979164    11.979173    23.889999    75.200935\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    3645\n",
      "HIGH      2063\n",
      "LOW         63\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    63.160631\n",
      "HIGH      35.747704\n",
      "LOW        1.091665\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_20241101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 7 matching daily pairs\n",
      "\n",
      "Processing daily pair for 20250419\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 5340.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 3242 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll            date\n",
      "0  37.979164   8.145839  15.990000     0.261728  Daily_20250419\n",
      "1  37.979164   8.187506  17.049999     0.267158  Daily_20250419\n",
      "2  37.979164   8.229173  17.094999     0.268612  Daily_20250419\n",
      "3  37.979164   8.270839  17.195000     0.254668  Daily_20250419\n",
      "4  37.979164   8.312506  17.215000     0.249075  Daily_20250419\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  3242.000000  3242.000000  3242.000000  3242.000000\n",
      "mean     36.392516    10.888989    17.883120     0.487192\n",
      "std       1.240946     1.001598     0.952558     1.543923\n",
      "min      33.187500     8.145839    15.740000     0.087971\n",
      "25%      35.697916    10.604173    17.150000     0.148618\n",
      "50%      36.729164    11.187506    17.684999     0.177824\n",
      "75%      37.395832    11.604173    18.275000     0.256980\n",
      "max      37.979164    11.979173    21.404999    23.104805\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    1933\n",
      "LOW        843\n",
      "HIGH       466\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    59.623689\n",
      "LOW       26.002468\n",
      "HIGH      14.373843\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Daily_20250419.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing daily pair for 20250418\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4491.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 3675 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll            date\n",
      "0  37.979164   7.020839  16.734999     0.216179  Daily_20250418\n",
      "1  37.979164   7.062506  16.734999     0.205943  Daily_20250418\n",
      "2  37.979164   7.104172  16.834999     0.212590  Daily_20250418\n",
      "3  37.979164   7.145839  16.730000     0.224532  Daily_20250418\n",
      "4  37.979164   7.187506  16.785000     0.231402  Daily_20250418\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  3675.000000  3675.000000  3675.000000  3675.000000\n",
      "mean     36.255186    10.650590    17.559948     0.492123\n",
      "std       1.531576     1.303396     1.450778     1.335603\n",
      "min      33.020832     7.020839    13.929999     0.089045\n",
      "25%      34.687500    10.187506    16.504999     0.150391\n",
      "50%      37.062500    11.145839    16.955000     0.191990\n",
      "75%      37.583332    11.604173    18.612499     0.235812\n",
      "max      37.979164    11.979173    21.809999    17.274824\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    2013\n",
      "LOW       1140\n",
      "HIGH       522\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    54.775510\n",
      "LOW       31.020408\n",
      "HIGH      14.204082\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Daily_20250418.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing daily pair for 20250417\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 3509.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 3289 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll            date\n",
      "0  37.979164   7.187506  17.125000     0.206933  Daily_20250417\n",
      "1  37.979164   7.229172  16.895000     0.205385  Daily_20250417\n",
      "2  37.979164   7.270839  16.529999     0.201602  Daily_20250417\n",
      "3  37.979164   7.312506  17.004999     0.198594  Daily_20250417\n",
      "4  37.979164   7.354172  17.100000     0.201561  Daily_20250417\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  3289.000000  3289.000000  3289.000000  3289.000000\n",
      "mean     35.652812    10.616524    17.532324     0.607799\n",
      "std       1.523826     1.313105     1.274796     1.279850\n",
      "min      33.020832     7.020839    14.380000     0.036085\n",
      "25%      34.270832    10.229173    16.484999     0.154481\n",
      "50%      35.562500    11.104173    17.285000     0.186719\n",
      "75%      37.229164    11.520839    18.285000     0.285630\n",
      "max      37.979164    11.979173    21.740000    14.562012\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    1682\n",
      "LOW        879\n",
      "HIGH       728\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    51.140164\n",
      "LOW       26.725448\n",
      "HIGH      22.134387\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Daily_20250417.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing daily pair for 20250416\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 6251.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 0 valid data points\n",
      "\n",
      "First few rows:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Data statistics:\n",
      "Error extracting data to DataFrame: Cannot describe a DataFrame without columns\n",
      "\n",
      "Processing daily pair for 20250415\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 4681.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 2530 valid data points\n",
      "\n",
      "First few rows:\n",
      "    latitude  longitude        sst  chlorophyll            date\n",
      "0  37.979164   7.020839  17.334999     0.210796  Daily_20250415\n",
      "1  37.979164   7.062506  17.334999     0.210964  Daily_20250415\n",
      "2  37.979164   7.104172  17.369999     0.203291  Daily_20250415\n",
      "3  37.979164   7.145839  17.369999     0.194784  Daily_20250415\n",
      "4  37.979164   7.187506  17.180000     0.188726  Daily_20250415\n",
      "\n",
      "Data statistics:\n",
      "          latitude    longitude          sst  chlorophyll\n",
      "count  2530.000000  2530.000000  2530.000000  2530.000000\n",
      "mean     36.886840     9.266837    18.468399     0.430439\n",
      "std       1.381624     1.358763     1.198681     1.801915\n",
      "min      33.395832     7.020839    15.705000     0.145752\n",
      "25%      37.104164     8.104173    17.520000     0.189572\n",
      "50%      37.437500     9.145839    18.250000     0.209890\n",
      "75%      37.729164    10.354173    19.295000     0.256091\n",
      "max      37.979164    11.979173    22.855000    57.078804\n",
      "\n",
      "Checking for infinite values:\n",
      "sst            0\n",
      "chlorophyll    0\n",
      "dtype: int64\n",
      "\n",
      "Classifying fishing zones...\n",
      "\n",
      "Class distribution:\n",
      "FishingZone\n",
      "MEDIUM    2158\n",
      "HIGH       366\n",
      "LOW          6\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "FishingZone\n",
      "MEDIUM    85.296443\n",
      "HIGH      14.466403\n",
      "LOW        0.237154\n",
      "Name: count, dtype: float64\n",
      "Saved DataFrame to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Daily_20250415.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing daily pair for 20250414\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 3169.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 0 valid data points\n",
      "\n",
      "First few rows:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Data statistics:\n",
      "Error extracting data to DataFrame: Cannot describe a DataFrame without columns\n",
      "\n",
      "Processing daily pair for 20250413\n",
      "Data shapes:\n",
      "SST shape: (192, 120)\n",
      "Chlorophyll shape: (192, 120)\n",
      "Latitude shape: (192,)\n",
      "Longitude shape: (120,)\n",
      "Processing data points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 192/192 [00:00<00:00, 3324.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 0 valid data points\n",
      "\n",
      "First few rows:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Data statistics:\n",
      "Error extracting data to DataFrame: Cannot describe a DataFrame without columns\n",
      "\n",
      "--- STEP 4: ANALYZING DATA AND CALCULATING PFZ ---\n",
      "\n",
      "Analyzing Monthly data for 20240301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240301.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20240101\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240101.png\n",
      "Saved top 1 potential fishing zones to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Locations_Monthly_20240101.csv\n",
      "\n",
      "Top 5 Potential Fishing Zones:\n",
      "Location 1: Lat 35.1708°N, Lon 11.1208°E (PFZ Index: 0.7138)\n",
      "\n",
      "Analyzing Monthly data for 20240501\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240501.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20240401\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240401.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20240601\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240601.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20240701\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240701.png\n",
      "Saved top 1 potential fishing zones to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Locations_Monthly_20240701.csv\n",
      "\n",
      "Top 5 Potential Fishing Zones:\n",
      "Location 1: Lat 37.1708°N, Lon 9.8208°E (PFZ Index: 0.7014)\n",
      "\n",
      "Analyzing Monthly data for 20240801\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240801.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20241001\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20241001.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20240901\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20240901.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20241201\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20241201.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Monthly data for 20241101\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Monthly_20241101.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Daily data for 20250419\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Daily_20250419.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Daily data for 20250418\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Daily_20250418.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Daily data for 20250417\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Daily_20250417.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "Analyzing Daily data for 20250415\n",
      "Saved analysis figure to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Analysis_Daily_20250415.png\n",
      "\n",
      "No high-potential fishing zones identified.\n",
      "\n",
      "--- STEP 5: CREATING SUMMARY REPORTS ---\n",
      "Saved monthly summary to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/Monthly_PFZ_Summary.csv\n",
      "Saved Monthly trends visualization\n",
      "Saved daily summary to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/Daily_PFZ_Summary.csv\n",
      "Saved Daily trends visualization\n",
      "\n",
      "--- STEP 6: TRAINING PREDICTION MODEL ---\n",
      "\n",
      "--- TRAINING PFZ PREDICTION MODEL ---\n",
      "Combined dataset has 63348 data points\n",
      "Training set size: 50678 samples\n",
      "Testing set size: 12670 samples\n",
      "\n",
      "Class distribution in training set:\n",
      "Label\n",
      "0    23833\n",
      "1    20821\n",
      "2     6024\n",
      "Name: count, dtype: int64\n",
      "Percentage: Label\n",
      "0    47.028296\n",
      "1    41.084889\n",
      "2    11.886815\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Class distribution in test set:\n",
      "Label\n",
      "0    5958\n",
      "1    5206\n",
      "2    1506\n",
      "Name: count, dtype: int64\n",
      "Percentage: Label\n",
      "0    47.024467\n",
      "1    41.089187\n",
      "2    11.886346\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Training Random Forest Classifier (simplified version)...\n",
      "Random Forest Accuracy: 0.7129\n",
      "Random Forest F1 Score (weighted): 0.6982\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.70      0.85      0.77      5958\n",
      "      Medium       0.73      0.69      0.71      5206\n",
      "        High       0.65      0.26      0.38      1506\n",
      "\n",
      "    accuracy                           0.71     12670\n",
      "   macro avg       0.70      0.60      0.62     12670\n",
      "weighted avg       0.71      0.71      0.70     12670\n",
      "\n",
      "\n",
      "Training Neural Network (simplified version)...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6862 - loss: 0.8142 - val_accuracy: 0.6995 - val_loss: 0.7644\n",
      "Epoch 2/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 0.7541 - val_accuracy: 0.7021 - val_loss: 0.7552\n",
      "Epoch 3/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 0.7552 - val_accuracy: 0.7034 - val_loss: 0.7473\n",
      "Epoch 4/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7057 - loss: 0.7450 - val_accuracy: 0.7014 - val_loss: 0.7457\n",
      "Epoch 5/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7019 - loss: 0.7433 - val_accuracy: 0.7012 - val_loss: 0.7413\n",
      "Epoch 6/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7063 - loss: 0.7361 - val_accuracy: 0.7005 - val_loss: 0.7358\n",
      "Epoch 7/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7019 - loss: 0.7352 - val_accuracy: 0.7008 - val_loss: 0.7309\n",
      "Epoch 8/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7016 - loss: 0.7327 - val_accuracy: 0.7038 - val_loss: 0.7275\n",
      "Epoch 9/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7027 - loss: 0.7274 - val_accuracy: 0.7030 - val_loss: 0.7228\n",
      "Epoch 10/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7066 - loss: 0.7186 - val_accuracy: 0.7043 - val_loss: 0.7180\n",
      "Epoch 11/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7095 - loss: 0.7138 - val_accuracy: 0.7050 - val_loss: 0.7148\n",
      "Epoch 12/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7109 - loss: 0.7120 - val_accuracy: 0.7010 - val_loss: 0.7138\n",
      "Epoch 13/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7076 - loss: 0.7102 - val_accuracy: 0.7042 - val_loss: 0.7086\n",
      "Epoch 14/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 0.7163 - val_accuracy: 0.7041 - val_loss: 0.7129\n",
      "Epoch 15/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7111 - loss: 0.7049 - val_accuracy: 0.7022 - val_loss: 0.7086\n",
      "Epoch 16/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7069 - loss: 0.7062 - val_accuracy: 0.7044 - val_loss: 0.7042\n",
      "Epoch 17/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7057 - loss: 0.7079 - val_accuracy: 0.7041 - val_loss: 0.7119\n",
      "Epoch 18/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7038 - loss: 0.7069 - val_accuracy: 0.7025 - val_loss: 0.7043\n",
      "Epoch 19/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7049 - loss: 0.7009 - val_accuracy: 0.7032 - val_loss: 0.7040\n",
      "Epoch 20/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7107 - loss: 0.6987 - val_accuracy: 0.7034 - val_loss: 0.7009\n",
      "Epoch 21/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7056 - loss: 0.7021 - val_accuracy: 0.7050 - val_loss: 0.6999\n",
      "Epoch 22/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7033 - loss: 0.7084 - val_accuracy: 0.7031 - val_loss: 0.6985\n",
      "Epoch 23/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7048 - loss: 0.6970 - val_accuracy: 0.7012 - val_loss: 0.6989\n",
      "Epoch 24/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7033 - loss: 0.6999 - val_accuracy: 0.7058 - val_loss: 0.6973\n",
      "Epoch 25/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 0.6977 - val_accuracy: 0.7045 - val_loss: 0.6949\n",
      "Epoch 26/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7029 - loss: 0.7051 - val_accuracy: 0.7035 - val_loss: 0.6949\n",
      "Epoch 27/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 0.6957 - val_accuracy: 0.6998 - val_loss: 0.7008\n",
      "Epoch 28/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7050 - loss: 0.6984 - val_accuracy: 0.7040 - val_loss: 0.6925\n",
      "Epoch 29/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7055 - loss: 0.6980 - val_accuracy: 0.7018 - val_loss: 0.6945\n",
      "Epoch 30/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6998 - loss: 0.7025 - val_accuracy: 0.7025 - val_loss: 0.6918\n",
      "Epoch 31/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7038 - loss: 0.6969 - val_accuracy: 0.7024 - val_loss: 0.6957\n",
      "Epoch 32/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7110 - loss: 0.6862 - val_accuracy: 0.7022 - val_loss: 0.6918\n",
      "Epoch 33/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7090 - loss: 0.6907 - val_accuracy: 0.7041 - val_loss: 0.6917\n",
      "Epoch 34/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7035 - loss: 0.6951 - val_accuracy: 0.7047 - val_loss: 0.6914\n",
      "Epoch 35/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7037 - loss: 0.6942 - val_accuracy: 0.7035 - val_loss: 0.6928\n",
      "Epoch 36/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7065 - loss: 0.6925 - val_accuracy: 0.7048 - val_loss: 0.6908\n",
      "Epoch 37/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7105 - loss: 0.6822 - val_accuracy: 0.7041 - val_loss: 0.6917\n",
      "Epoch 38/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7067 - loss: 0.6874 - val_accuracy: 0.7001 - val_loss: 0.6880\n",
      "Epoch 39/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7066 - loss: 0.6917 - val_accuracy: 0.7043 - val_loss: 0.6915\n",
      "Epoch 40/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7002 - loss: 0.6979 - val_accuracy: 0.7010 - val_loss: 0.6879\n",
      "Epoch 41/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7044 - loss: 0.6929 - val_accuracy: 0.6993 - val_loss: 0.6913\n",
      "Epoch 42/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.6884 - val_accuracy: 0.7054 - val_loss: 0.6924\n",
      "Epoch 43/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7009 - loss: 0.7001 - val_accuracy: 0.7031 - val_loss: 0.6880\n",
      "Epoch 44/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7047 - loss: 0.6899 - val_accuracy: 0.7048 - val_loss: 0.6866\n",
      "Epoch 45/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7036 - loss: 0.6948 - val_accuracy: 0.7034 - val_loss: 0.6869\n",
      "Epoch 46/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7024 - loss: 0.6935 - val_accuracy: 0.7030 - val_loss: 0.6875\n",
      "Epoch 47/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7082 - loss: 0.6849 - val_accuracy: 0.7039 - val_loss: 0.6879\n",
      "Epoch 48/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7078 - loss: 0.6856 - val_accuracy: 0.7012 - val_loss: 0.6929\n",
      "Epoch 49/50\n",
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7135 - loss: 0.6814 - val_accuracy: 0.7037 - val_loss: 0.6867\n",
      "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7079 - loss: 0.6833\n",
      "Neural Network Accuracy: 0.7103\n",
      "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Neural Network F1 Score (weighted): 0.6947\n",
      "\n",
      "Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.73      0.78      0.76      5958\n",
      "      Medium       0.68      0.77      0.72      5206\n",
      "        High       0.76      0.23      0.36      1506\n",
      "\n",
      "    accuracy                           0.71     12670\n",
      "   macro avg       0.72      0.59      0.61     12670\n",
      "weighted avg       0.71      0.71      0.69     12670\n",
      "\n",
      "Saved classification reports to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/PFZ_Classification_Reports.txt\n",
      "Saved Random Forest model to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Models/random_forest_model.joblib\n",
      "Saved Neural Network model to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Models/neural_network_model.keras\n",
      "\n",
      "The best model is: RandomForest\n",
      "\n",
      "--- STEP 7: GENERATING PREDICTIONS FOR VALIDATION ---\n",
      "\n",
      "Validating model on 20240101 data...\n",
      "\n",
      "Generating PFZ predictions for Monthly_20240101...\n",
      "Prediction accuracy: 0.1810\n",
      "F1 score (weighted): 0.2183\n",
      "Saved prediction results to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Predictions_Monthly_20240101.csv\n",
      "Saved high potential fishing zones map to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/High_PFZ_Monthly_20240101.png\n",
      "\n",
      "Validating model on 20240601 data...\n",
      "\n",
      "Generating PFZ predictions for Monthly_20240601...\n",
      "Prediction accuracy: 0.3901\n",
      "F1 score (weighted): 0.4372\n",
      "Saved prediction results to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Predictions_Monthly_20240601.csv\n",
      "Saved high potential fishing zones map to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/High_PFZ_Monthly_20240601.png\n",
      "\n",
      "Validating model on 20241201 data...\n",
      "\n",
      "Generating PFZ predictions for Monthly_20241201...\n",
      "Prediction accuracy: 0.3876\n",
      "F1 score (weighted): 0.4727\n",
      "Saved prediction results to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/processed/Tunisia_PFZ_Predictions_Monthly_20241201.csv\n",
      "Saved high potential fishing zones map to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/High_PFZ_Monthly_20241201.png\n",
      "Saved validation results to /content/drive/MyDrive/Tunisia_PFZ_Model/Results/Analysis/Model_Validation_Results.csv\n",
      "\n",
      "Tunisia PFZ Model processing complete!\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# SECTION 10: MAIN EXECUTION\n",
    "#############################################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Step 1: Read all satellite data\n",
    "    print(\"\\n--- STEP 1: READING SATELLITE DATA ---\")\n",
    "    sst_mo_data, chl_mo_data, sst_dl_data, chl_dl_data = read_all_data()\n",
    "\n",
    "    # Step 2: Crop data to Tunisia region\n",
    "    print(\"\\n--- STEP 2: CROPPING DATA TO TUNISIA REGION ---\")\n",
    "    sst_mo_tunisia, chl_mo_tunisia, sst_dl_tunisia, chl_dl_tunisia = crop_all_data(\n",
    "        sst_mo_data, chl_mo_data, sst_dl_data, chl_dl_data\n",
    "    )\n",
    "\n",
    "    # Step 3: Process paired data\n",
    "    print(\"\\n--- STEP 3: PROCESSING PAIRED DATA ---\")\n",
    "    monthly_dfs, daily_dfs = process_paired_data(\n",
    "        sst_mo_tunisia, chl_mo_tunisia, sst_dl_tunisia, chl_dl_tunisia\n",
    "    )\n",
    "\n",
    "    # Step 4: Analyze data and calculate PFZ\n",
    "    print(\"\\n--- STEP 4: ANALYZING DATA AND CALCULATING PFZ ---\")\n",
    "\n",
    "    # Process monthly data\n",
    "    monthly_results = []\n",
    "    for date, df in monthly_dfs.items():\n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"\\nAnalyzing Monthly data for {date}\")\n",
    "            pfz_index = calculate_pfz_index(df, f\"Monthly_{date}\")\n",
    "\n",
    "            if pfz_index is not None:\n",
    "                monthly_results.append({\n",
    "                    'date': date,\n",
    "                    'data_points': len(df),\n",
    "                    'sst_min': df['sst'].min(),\n",
    "                    'sst_max': df['sst'].max(),\n",
    "                    'chl_min': df['chlorophyll'].min(),\n",
    "                    'chl_max': df['chlorophyll'].max()\n",
    "                })\n",
    "\n",
    "    # Process daily data\n",
    "    daily_results = []\n",
    "    for date, df in daily_dfs.items():\n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"\\nAnalyzing Daily data for {date}\")\n",
    "            pfz_index = calculate_pfz_index(df, f\"Daily_{date}\")\n",
    "\n",
    "            if pfz_index is not None:\n",
    "                daily_results.append({\n",
    "                    'date': date,\n",
    "                    'data_points': len(df),\n",
    "                    'sst_min': df['sst'].min(),\n",
    "                    'sst_max': df['sst'].max(),\n",
    "                    'chl_min': df['chlorophyll'].min(),\n",
    "                    'chl_max': df['chlorophyll'].max()\n",
    "                })\n",
    "\n",
    "    # Step 5: Create summary reports\n",
    "    print(\"\\n--- STEP 5: CREATING SUMMARY REPORTS ---\")\n",
    "\n",
    "    # Monthly summary\n",
    "    if monthly_results:\n",
    "        monthly_df = pd.DataFrame(monthly_results)\n",
    "        monthly_summary_path = os.path.join(analysis_dir, 'Monthly_PFZ_Summary.csv')\n",
    "        monthly_df.to_csv(monthly_summary_path, index=False)\n",
    "        print(f\"Saved monthly summary to {monthly_summary_path}\")\n",
    "\n",
    "        # Plot monthly trends\n",
    "        plot_time_series(monthly_df, 'Monthly')\n",
    "\n",
    "    # Daily summary\n",
    "    if daily_results:\n",
    "        daily_df = pd.DataFrame(daily_results)\n",
    "        daily_summary_path = os.path.join(analysis_dir, 'Daily_PFZ_Summary.csv')\n",
    "        daily_df.to_csv(daily_summary_path, index=False)\n",
    "        print(f\"Saved daily summary to {daily_summary_path}\")\n",
    "\n",
    "        # Plot daily trends if there are enough points\n",
    "        if len(daily_df) >= 3:\n",
    "            plot_time_series(daily_df, 'Daily')\n",
    "\n",
    "    # Step 6: Train prediction model\n",
    "    print(\"\\n--- STEP 6: TRAINING PREDICTION MODEL ---\")\n",
    "    best_model, predict_function, model_type = train_pfz_prediction_model(monthly_dfs)\n",
    "\n",
    "    # Step 7: Generate predictions for validation\n",
    "    print(\"\\n--- STEP 7: GENERATING PREDICTIONS FOR VALIDATION ---\")\n",
    "    validation_results = []\n",
    "\n",
    "    # Select a few months for validation\n",
    "    validation_months = ['20240101', '20240601', '20241201']  # Winter, Summer, Winter\n",
    "    for month in validation_months:\n",
    "        if month in monthly_dfs and monthly_dfs[month] is not None:\n",
    "            print(f\"\\nValidating model on {month} data...\")\n",
    "            accuracy, f1 = generate_pfz_predictions(\n",
    "                best_model, predict_function, model_type,\n",
    "                monthly_dfs[month], f\"Monthly_{month}\"\n",
    "            )\n",
    "            validation_results.append({\n",
    "                'date': month,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1\n",
    "            })\n",
    "\n",
    "    # Save validation results\n",
    "    if validation_results:\n",
    "        validation_df = pd.DataFrame(validation_results)\n",
    "        validation_path = os.path.join(analysis_dir, 'Model_Validation_Results.csv')\n",
    "        validation_df.to_csv(validation_path, index=False)\n",
    "        print(f\"Saved validation results to {validation_path}\")\n",
    "\n",
    "        # Plot validation metrics\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = np.arange(len(validation_df))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.bar(x - width/2, validation_df['accuracy'], width, label='Accuracy')\n",
    "        plt.bar(x + width/2, validation_df['f1_score'], width, label='F1 Score')\n",
    "\n",
    "        plt.xlabel('Validation Month')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Validation Metrics')\n",
    "        plt.xticks(x, validation_df['date'])\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        val_path = os.path.join(analysis_dir, 'Validation_Metrics.png')\n",
    "        plt.savefig(val_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\nTunisia PFZ Model processing complete!\")\n",
    "\n",
    "def plot_time_series(df, data_type):\n",
    "    \"\"\"Create time series plots from summary data\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        # Convert date strings to datetime for proper ordering\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "        df = df.sort_values('date')\n",
    "\n",
    "        # SST min/max plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(df['date'], df['sst_min'], 'b-', marker='o', label='Min SST')\n",
    "        plt.plot(df['date'], df['sst_max'], 'r-', marker='o', label='Max SST')\n",
    "        plt.fill_between(df['date'], df['sst_min'], df['sst_max'], alpha=0.2, color='purple')\n",
    "        plt.title(f'{data_type} Sea Surface Temperature Trend')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Temperature (°C)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "        # Chlorophyll min/max plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(df['date'], df['chl_min'], 'g-', marker='o', label='Min Chlorophyll')\n",
    "        plt.plot(df['date'], df['chl_max'], 'm-', marker='o', label='Max Chlorophyll')\n",
    "        plt.fill_between(df['date'], df['chl_min'], df['chl_max'], alpha=0.2, color='olive')\n",
    "        plt.title(f'{data_type} Chlorophyll Concentration Trend')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Chlorophyll (mg/m³)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(analysis_dir, f'{data_type}_PFZ_Trends.png'), dpi=300)\n",
    "        print(f\"Saved {data_type} trends visualization\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating time series plots: {e}\")\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7G91Ozx+W+Dh+BfwjkM2y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
